Co je to n-gram?
-sekvence ğ‘› po sobÄ› jdoucÃ­ch slov nebo znakÅ¯ v textu.

Jak se poÄÃ­tajÃ­ frekvence unigramÅ¯, bigramÅ¯ a trigramÅ¯?
- poÄet vÃ½skytÅ¯ danÃ©ho n-gramu v textu

Jak se poÄÃ­tÃ¡ pravdÄ›podobnost vÃ½skytu slova v rÃ¡mci n-gramovÃ©ho modelu?
- PodmÃ­nÄ›nÃ¡ pravdÄ›podobnost vÃ½skytu slova ğ‘¤ po kontextu k. Tedy napÅ™ "jÃ¡ jsem" P("jsem"|"jÃ¡") = poÄet vÃ½skÃ½tÅ¯ vÅ¡ech ("jÃ¡ jsem") / poÄet vÃ½skytu ("jÃ¡)

Co je Laplaceovo vyhlazovÃ¡nÃ­ a proÄ se pouÅ¾Ã­vÃ¡?
- Laplaceovo vyhlazovÃ¡nÃ­ pÅ™idÃ¡vÃ¡ k poÄÃ­tÃ¡nÃ­ pravdÄ›podobnostÃ­ malou hodnotu (napÅ™. 1), aby se zabrÃ¡nilo nulovÃ½m pravdÄ›podobnostem u nevidÄ›nÃ½ch n-gramÅ¯.

Jak lze pouÅ¾Ã­t bigramovÃ½ model pro automatickÃ© doplÅˆovÃ¡nÃ­ slov?
- Na zÃ¡kladÄ› pÅ™edchozÃ­ho slova vybereme nejpravdÄ›podobnÄ›jÅ¡Ã­ nÃ¡sledujÃ­cÃ­ slovo podle pravdÄ›podobnosti z bigramovÃ©ho modelu.

Jak se urÄuje nejpravdÄ›podobnÄ›jÅ¡Ã­ nÃ¡sledujÃ­cÃ­ slovo?
- Vybereme slovo s nejvyÅ¡Å¡Ã­ pravdÄ›podobnostÃ­ napÅ™ P("jsem"|"jÃ¡") = 0,9 a P("umÃ­m"|"jÃ¡") = 0,1

Jak lze trigramovÃ½ model vyuÅ¾Ã­t pro generovÃ¡nÃ­ textu?

Jak lze vylepÅ¡it kvalitu generovanÃ©ho textu?
- nevybÃ­rat nejpravdÄ›podobnÄ›jÅ¡Ã­ slovo ale napÅ™ nÃ¡hodnÄ› aÅ¥ se text neopakuje.

Co je to perplexity a jak se vypoÄÃ­tÃ¡vÃ¡?
- Perplexity mÄ›Å™Ã­, jak dobÅ™e model pÅ™edpovÃ­dÃ¡ testovacÃ­ data. NiÅ¾Å¡Ã­ hodnota znamenÃ¡ lepÅ¡Ã­ model

JakÃ¡ je interpretace hodnoty perplexity pro jazykovÃ½ model?
NÃ­zkÃ¡ perplexity â†’ model lÃ©pe pÅ™edpovÃ­dÃ¡ slova.
VysokÃ¡ perplexity â†’ model je Å¡patnÃ½ (Äasto narÃ¡Å¾Ã­ na neznÃ¡mÃ¡ slova).

Jak se perplexity mÄ›nÃ­ s rostoucÃ­ velikostÃ­ n-gramu?
Unigramy: vysokÃ¡ perplexity, protoÅ¾e nepouÅ¾Ã­vajÃ­ kontext.
Bigramy: niÅ¾Å¡Ã­ perplexity, protoÅ¾e berou v Ãºvahu pÅ™edchozÃ­ slovo.
Trigramy: Äasto niÅ¾Å¡Ã­ perplexity neÅ¾ bigramy, ale pokud je mÃ¡lo dat, mohou mÃ­t vyÅ¡Å¡Ã­ perplexity kvÅ¯li neznÃ¡mÃ½m kombinacÃ­m slov. 